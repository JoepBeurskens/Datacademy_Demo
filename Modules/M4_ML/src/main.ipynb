{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Science Bootcamp Basics 2 - ML: Machine Learning </h1>\n",
    "<hr>\n",
    "\n",
    "<p>\n",
    "    Throughout this module you will be working through the CRISP-DM model, as shown in the Easy-LMS course. <br>\n",
    "    The CRISP-DM model clearly defines the phases of design, creation and evaluation of a Machine Learning (ML) model. <br>\n",
    "    For this module, the business understanding and deployment phases are excluded, as they are not suited for this task. <br>\n",
    "    You will however learn to deploy your models towards the finale module of this Bootcamp! <br>\n",
    "    <br>\n",
    "    You will be working with a simple, but well-known, dataset called the Iris dataset. <br>\n",
    "    This dataset consists of 150 samples of flowers, described using the following characteristics:\n",
    "    <ol>\n",
    "        <li>\n",
    "            <b><u>Iris dataset</u></b>\n",
    "            <ul>\n",
    "                <li> <b>id</b>: The unique ID reference of the properties. </li>\n",
    "                <li> <b>sepal length (cm)</b>: The length of the outer part of an iris flower that surround the petals. </li>\n",
    "                <li> <b>sepal width (mm)</b>: The width of the outer part of an iris flower that surround the petals. </li>\n",
    "                <li> <b>petal length (cm)</b>: The length of the parts of an iris flower that are often conspicuously colored. </li>\n",
    "                <li> <b>petal width (cm)</b>: The width of the parts of an iris flower that are often conspicuously colored. </li>\n",
    "                <li> <b>state</b>: How moist the plant felt at the time of measurement. </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>A. Data understanding </h3>\n",
    "<p>\n",
    "We will use the Iris dataset to <b>analyze</b>, <b>classify</b> and <b>predict</b> different Iris plant species. <br>\n",
    "As this is not a task at a company but just for practice, we skip the business understanding and dive directly into the data. <br>\n",
    "The <b>Data Understanding Phase</b> is executed to gain insight into the characteristics of the dataset. <br>\n",
    "A first glance on the data might reveal weaknesses, and could tell you which preprocessing steps are needed in the <b>Data Preparation Phase</b>.<br>\n",
    "<br>\n",
    "To help you get started, we already created some code for you which can be imported. <br>\n",
    "The pre-written code and Iris dataset are all enclosed within a class, called <b>Function</b>. <br>\n",
    "If the <b>Function</b> class needs to be used, this will be explained in advance of the assignment. <br>\n",
    "Usually, this will take the following form: <code>func.<i>[SUGGESTED FUNCTION HERE]</i>(<i>[SOME VALUES]</i>)</code>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../../..\n",
    "from Modules.M4_ML.libs.function import Function\n",
    "func = Function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within Python the mosts used library to work with data is called <b>Pandas</b>. <br>\n",
    "Pandas saves data in a so-called <b>DataFrame</b>, which can be thought of as an SQL table or an Excel worksheet. <br>\n",
    "Such DataFrames can be created manually, however in most cases this is not needed. <br>\n",
    "Pandas allows to read a DataFrame from for example a CSV or Excel file, which prevents the need to define it yourself. <br>\n",
    "<br>\n",
    "For this assignment you can retrieve the DataFrame from the <b>Function</b> class, as the data is integrated in the <code>func.iris</code> variable. <br>\n",
    "Normally, you save a DataFrame to a local variable, an often used abbreviation is <code>df</code>, after which the variable can be used for all other steps. <br>\n",
    "However to allow the eventual tests to run and check your code, you will work with the data from within the class reference. <br>\n",
    "What this means is that you will refer to the DataFrame by calling <code>func.iris</code> instead of <code>df</code> as you would normally do. <br>\n",
    "Below the data will be printed by simply calling the class variable <code>func.iris</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>Data Understanding Phase</b> is used to gather a great understanding of the dataset to be used. <br>\n",
    "This process is often described as <b>Exploratory Data Analysis (EDA)</b>, which is all about making sense of the data. <br>\n",
    "Starting this process can be done using the in-build functions of Pandas, of which the most important will be described below. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7> <b> -- A1. Pandas DataFrame basic statistics -- </b> </h7>\n",
    "\n",
    "These functionss can be used to get a first glimpse of the characteristics of the Iris dataset. <br>\n",
    "\n",
    "<ul>\n",
    "    <li> <b><code>[DATAFRAME].shape</code></b> - Returns the dimensionality of the DataFrame. </li>\n",
    "    <li> <b><code>[DATAFRAME].head(<i>n</i>) </code></b> - Returns the first <i>n</i> of the base DataFrame. </li>\n",
    "    <li> <b><code>[DATAFRAME].info()</code></b> - Returns a summary describing the DataFrame, containing the index, columns, non-null values and the datatype. </li>\n",
    "    <li> <b><code>[DATAFRAME].isnull().sum()</code></b> - Returns the number of zero values in each column. </li>\n",
    "</ul>\n",
    "\n",
    "If you desire to extend your knowledge, which might be usefull for later exercises, look through the <a href=\"https://pandas.pydata.org/docs/reference/frame.html\">Pandas documentation</a> for all the modules in this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of your table (DataFrame)\n",
    "func.iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first (n) 5 rows from the dataset\n",
    "func.iris.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any inconsistency in the data\n",
    "func.iris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many zero values\n",
    "func.iris.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides these basic analyses, additional analysis can be executed to discover patterns, spot anomalies and to check assumptions. <br>\n",
    "Doing this can be done in different ways, both using summary statistics as with graphical respresentations. <br>\n",
    "Again Pandas has some in-build functions for this, of which the most important will be described below. <br>\n",
    " \n",
    "<!-- <ul>\n",
    "    <li> <b>Statistics</b>: Describe the data statistics, including the central tendency, dispersion and shape of a dataset distribution, excluding <code>NaN</code> values. </li>\n",
    "    <li> <b>Histograms</b>:  Represent the distribution of the data in the DataFrame. </li>\n",
    "    <li> <b>Boxplots</b>: Visualize the distribution of quantitative data in a way that facilitates comparisons between variables. The box shows the range from the first to the third quartile, with the median displayed as the line in the middle.\n",
    "    The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution. A segment inside the rectangle shows the median and “whiskers” above and below the box show the locations of the minimum and maximum. Outliers are either <math>1.5×Inter Quartile Range</math> or more above the third quartile or more below the first quartile. </li>\n",
    "    <li> <b>Correlation Matrix</b>: To use linear regression for modelling, its necessary to remove correlated variables to improve your model. One can find correlations using <code>pandas.corr()</code>. It’s a good practice to remove correlated variables during feature selection. </li>\n",
    "</ul> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7> <b> -- A2. Pandas DataFrame extended statistics -- </b> </h7>\n",
    "\n",
    "<ul>\n",
    "    <li><b><code>[DATAFRAME].describe()</code></b> - Returns the basic data statistics, including the count, mean, standard deviation, the quartiles and the minimum and maximum value. These statistics exclude the <b>NaN</b> values.  </li> \n",
    "    <li><b><code>[DATAFRAME].hist()</code></b> - Returns histograms of the data for all columns (parameters) separately. Showing the distribution of the data.</li>\n",
    "    <li><b><code>[DATAFRAME].boxplot()</code></b> - Returns boxplots of the data for all columns (parameters) separately. The box shows the range from the first to the third quartile, with the median displayed as the line in the middle. The total length between the first and third quartile (de length of the box) is defined as the Inter Quartile Range (IQR). The whiskers outside the box are set to be 1.5 times this IQR value, as this is perceived as the allowed minimum and maximum values. An outlier would be any point outside the whiskers, however as the boxplots only concern a quick measure this conclusion has to be made with caution. There are statistical tests to define whether a value is indeed an outlier or not, but the complexity of such tests exceed the purpose of this course. In most cases it suffices to rely on a soft measure, to be decided whether the distance between the whisker and the value is indeed significant.</li>\n",
    "    <li><b><code>[DATAFRAME].corr()</code></b> - Returns a matrix, displaying the correlations between all columns (parameters). As highly correlated values do not add any knowledge and just add complexity to your model, it is a good practice to remove these values during <b>feature selection</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics summary\n",
    "func.iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the summary statistics above, three conclusions can be drawn:\n",
    "<ul>\n",
    "    <li> There indeed are missing values in the <i>petal length (cm)</i> column, as the count shows a different value than in other columns due to the exclusion of <b>NaN</b> values.</li>\n",
    "    <li> The values for the <i>sepal width (mm)</i> column are significantly higher than the mean plus standard deviation of other columns, which might indicate a different measuring unit used. </li>\n",
    "    <li> The maximum value of the <i>sepal length (cm)</i> column shows quite high, this needs further investigation.\n",
    "</ul>\n",
    "In defining the histrogram we set a variable called <code>hist</code> in which the returning values of the histogram will be saved. <br>\n",
    "We will not use these values, but if not defined they will be printed above the plots which might be confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms\n",
    "hist = func.iris.hist(figsize=(16,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms above show the distribution of values for all parameters separately. <br>\n",
    "Immediately it can be seen that the distribution in the <i>sepal length (cm)</i> plot looks off. <br>\n",
    "Together with our conclusion of the <i>sepal length (cm)</i> from the summary statistics, this might indicate an outlier. <br>\n",
    "To define whether this hypothesis is true, a boxplot can give some deeper understanding. <br>\n",
    "<br>\n",
    "Here, we save the boxplot to a variable called <code>boxplot</code>. This because in the <code>[DataFrame].boxplot()</code> we have the option to save returning values to (in this case) a <code>'dict'</code> (dictionary). It contains the Lines making up the boxes, caps, fliers, medians, and whiskers. This may come in handy when we want to further investigate one of these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "boxplot = func.iris.boxplot(figsize=(16,8), return_type='dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the boxplot of the <i>sepal length (cm)</i>, the maximum point we distinguished earlier is shown to be way above the whiskers. <br>\n",
    "This indicates that this value, as it shows significant distance from the whiskers, concerns an outlier. <br>\n",
    "The points outside the whiskers of the <i>sepal width (mm)</i> will be discussed later on. <br>\n",
    "<br>\n",
    "To see if we have strongly correlated variables in the dataset, we can use the correlation matrix. <br>\n",
    "Adding strongly correlated variables to a Machine Learning (ML) model increases the complexity. <br>\n",
    "The increased complexity will thereafter require more computing power to train and use the model. <br> \n",
    "When leaving one of the strongly correlated variables out of a ML model increases the computation effeciency (higher dimensionallity) and doesn't effect the predictive power of the model significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "func.iris.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps above conclude the <b>Exploratory Data Analysis (EDA)</b>, which is executed to gain knowledge about your data. <br>\n",
    "The gained knowledge from execution of the <b>Data Understanding</b> phase will be used in the consecutive phases. <br>\n",
    "It will mostly be the basis on which you decide which data preprocessing steps need to be executed in the next phase <b>Data Preparation</b>. <br>\n",
    "<br>\n",
    "To test whether the conclusion you made based on the EDA are correct, we ask you to fill in the following values. <br>\n",
    "After completing all assignments and pushing this repository you will be notified if your conclusions are correct with regards to the underlying data. <br>\n",
    "Mistakes in this step will propagate into the consecutive steps of the CRISP-DM model, which will influence your results. <br>\n",
    "Despite it being unfortunate to find a potential mistake so late in the process, if it were an actual project it would be the same. <br>\n",
    "However, not all code will have to be rewritten, as you can simply adjust some code and then rerun all following notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_missing_values_petal_length = ...  # give as integer\n",
    "average_value_sepal_width= ...   # give a float in two digits behind the decimal\n",
    "highly_correlated_columns= [...]   # give a list\n",
    "\n",
    "func.execute_function(exercise=\"A1\", answer=num_missing_values_petal_length)\n",
    "func.execute_function(exercise=\"A2\", answer=average_value_sepal_width)\n",
    "func.execute_function(exercise=\"A3\", answer=highly_correlated_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3> B. Data preparation </h3>\n",
    "<p>\n",
    "Before we apply Machine Learning (ML) algorithms to the data, we have to prepare it. <br>\n",
    "In the EDA we observed that there are missing values, outliers and a strong correlation between two columns. <br>\n",
    "However, besides the found characteristics, more steps can be needed to prepare the data. <br>\n",
    "The standard procedure consists of multiple so-called pillars, namely:\n",
    "<ol>\n",
    "    <li><b>Data integration</b> - Combining multiple data sources into one, handling data redundancy. </li>\n",
    "    <li><b>Data cleaning</b> - Remove outliers, impute missing values an handle data inconsistencies. </li>\n",
    "    <li><b>Data transformation</b> - Adjust numerical and categorical values to allow the model to work with them. </li>\n",
    "    <li><b>Data (feature) selection</b> - Select which values to include into your model. </li>\n",
    "    <li><b>Data reduction</b> - Simplify data where possible by for example producing aggregate values.</li>\n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7> <b> -- B1. Data Integration -- </b> </h7>\n",
    "\n",
    "If we would have had multiple datasets, this pillar would be used to join them into one. <br>\n",
    "Pandas has a function for this, which looks like this: <code>[DATAFRAME_1].merge([DATAFRAME_2])</code>. <br>\n",
    "As we only have data from a single source, we do not need to execute this data preparation pillar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7> <b> -- B2. Data Cleaning -- </b> </h7>\n",
    "\n",
    "The cleaning pillar consists of different parts which all need to be adressed, namely:\n",
    "<ul>\n",
    "    <li> <b>Outliers</b>: Ouliers can be detected by for example the previously used boxplot visualisation. Such outliers can either be entirely removed or the actual value can be updated (for example by replacing it with the mean).</li>\n",
    "    <li> <b>Missing values</b>: As mentioned in the introduction of Section B, missing values are detected. Such missing values can be imputed, with for example the mean or median, or completely removed. </li>\n",
    "    <ul>\n",
    "        <li> <i> Pro removal: </i> A model trained with the removal of all missing values creates a robust model. </li>\n",
    "        <li> <i> Con removal: </i> If removed, all the information that is in the other columns is removed as well. This especially works poorly if the percentage of missing values is excessive in comparison to the complete records. </li>       \n",
    "    </ul>\n",
    "    <li> <b>Inconsistent data</b>: The unit metric for one column in the dataset is different, for example the inconsistent use of <i> mm </i> or <i> cm </i>. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please <b>answer</b> the following questions:\n",
    "<ol>\n",
    "    <li> Provide a list of all the points outside the boxplots (outside the wiskers). </li>\n",
    "</ol>\n",
    "<i> Tips: use the saved variable boxplot to retrieve these points. outliers = [flier.get_ydata() for flier in boxplot[\"fliers\"]]</i>\n",
    "<ol start=\"2\">    \n",
    "    <li> Provide the actual outlier from the boxplot. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a list of all the points outside the boxplot\n",
    "outliers = [...]\n",
    "print(f\"The list of outliers is: {outliers}\")\n",
    "\n",
    "func.execute_function(exercise=\"B2-1\", answer=outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the actual outlier from the boxplot:\n",
    "actual_outlier = ...\n",
    "print(f\"Actual outlier: {actual_outlier}\")\n",
    "\n",
    "func.execute_function(exercise=\"B2-2\", answer=actual_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluding from the retrieved values, the maximum value of the <i>sepal length (cm)</i> is clearly an outlier. <br>\n",
    "Not only does it stand out in the visualisation, it also is an impossible length of a flower. <br>\n",
    "Looking at the value we might consider that by accident an additional 0 is added when inserting the measurements. <br>\n",
    "For this reason we decide to not remove the entire record, but we only remove the 0 to prevent data to be lost. <br>\n",
    "The other values in the <i>sepal width (mm)</i> boxplot are not found te be outliers. These values lie within <math>1.5 × IQR</math> and therefore not fall outside the overall pattern in the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the actual outlier\n",
    "func.remove_0_in_outlier(outlier_value=actual_outlier)\n",
    "\n",
    "# Check in de boxplot dat er geen outliers meer zijn\n",
    "cleaned_boxplot = func.iris.boxplot(figsize=(16,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we conclude the handling of <b>outliers</b>, we now continue to <b>missing values</b>.<br>\n",
    "An often used library to handle missing values is <b>Scikit-learn</b>, often abbreviated by <b>sklearn</b>. <br>\n",
    "<a href=\"https://scikit-learn.org/stable/\">Scikit-Learn</a> is an extensive Machine Learning (ML) library, available for Python. <br> \n",
    "This free ML library includes modules for the preparation of data and the design, training and use of actual ML models. <br>\n",
    "We recommend you get familiair with the <a href=\"https://scikit-learn.org/stable/\">Scikit-learn documentation</a>, as you will often use it in your Data Science ventures. <br>\n",
    "<br>\n",
    "For this module you will implement the k-Nearest Neighbors imputer for filling in the missing values. <br>\n",
    "Each missing value will be imputed with the mean of the <code>n</code> closest values found in the dataset. <br>\n",
    "Close in this sense is a distance measure of all values available for the concerning record.\n",
    "<!-- \n",
    "For this module we implement k-Nearest Neighbors imputer for completing missing values using. Each sample’s missing values are imputed using the mean value from <code>n_neighbors</code> nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Load and set the K-Nearest Neighbours imputer\n",
    "imputer = KNNImputer(n_neighbors=3, weights=\"uniform\")\n",
    "\n",
    "# Fit to the numerical data, then transform it and save the imputed values.\n",
    "func.iris.iloc[:, :-1] = imputer.fit_transform(func.iris.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The handling of missing values needs to happen intelligently to create a robust Machine Learning (ML) model. <br>\n",
    "Doing this can be done in several ways mostly depending on how and what the data is about, for example either categorical or numerical values. <br>\n",
    "Having domain knowledge about the dataset is crucial for this, as it gives insight into how preprocessing needs to be applied.<br>\n",
    "<br>\n",
    "Normally, if you apply imputation on your dataset, you should only use the training dataset to fit the transformer after applying it to both the train and test set. <br>\n",
    "This prevents the occurrence of <b>data leakage</b>, which is the phenomenon that knowledge that should not be available is available to the model. <br>\n",
    "A train-test split is used to train a model on the training set and then evaluate its performance on a test dataset that it did not see before. <br>\n",
    "If you allow the test data to be taken into account for imputation, some knowledge of the set that should be unknown is known to the trained ML model resulting in misleading evaluation of the model. <br>\n",
    "<br>\n",
    "Finally we handle a single <b>data inconsistency</b> that is in the measuring unit used to define the <i>sepal width (mm)</i> as opposed to other parameters. <br>\n",
    "The <i>sepal width (mm)</i> is given in <i>mm</i>, whereas the other parameters (<i>sepal length (cm)</i>, <i>petal length (cm)</i>, <i>petal width (cm)</i>) are given in <i>cm</i>. <br>\n",
    "<br>\n",
    "To deal with this do the following:\n",
    "<ol>\n",
    "    <li> Change the metric system used by dividing the <i>sepal width (<b>mm</b>)</i> column by 10.</li> \n",
    "    <li> And change the column name to the correct reference <i>sepal width (<b>cm</b>).</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform unit of the sepal width (mm) column to centimeters and rename it to sepal width (cm)\n",
    "# Multiply each value by 10\n",
    "func.iris['sepal width (mm)'] = ...\n",
    "\n",
    "# # And rename the column to 'sepal width (cm)'\n",
    "func.iris = ...\n",
    "\n",
    "func.execute_function(exercise=\"B2-3\", answer=func.iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7> <b> -- B3. Data Transformation -- </b> </h7>\n",
    "\n",
    "Data Transformation is applied to make the data more suitable for the underlying mathematics of the Machine Learning (ML) models. <br>\n",
    "Properly formatted data improves the quality of the deduced models, which will eventually lead to better outcomes. <br>\n",
    "The transformations that can be applied differ for different types of data, which are described below:\n",
    "<ul>\n",
    "    <li> <b>Categorical values</b> </li>\n",
    "    <ul>\n",
    "        <li> <b><i> Ordinal encoding: </i></b> - Assigns an integer value to each category in the order they are encountered. </li>\n",
    "        <ul>\n",
    "            <li> Only really useful if there exist a natural order in categories because the model will consider one category to be ‘higher’ or ‘closer’ to another if their assigned values are ‘higher’ or ‘closer’. </li>   \n",
    "        </ul>\n",
    "        <li> <b><i> One-hot encoding: </i></b> - Creates a column for all categories separately, assigning a 1 if that category applies to that row, otherwise assigning a 0. </li>\n",
    "        <ul>\n",
    "            <li> Can explode if a column has lots of categories, causing issues with high dimensionality.  </li>   \n",
    "        </ul> \n",
    "    </ul>\n",
    "    <br>\n",
    "    <li> <b>Numerical values</b> </li>\n",
    "    <ul>\n",
    "        <li> <b><i> Normalization:</i></b> - Changes the range between which the values lie. Examples are: <i>min-max, z-score, logarithmic</i> </li>\n",
    "        <li> <b><i> Discretize:</i></b> - Classify all values within certain bins, for example cutting the range from 0-10 into bins 1.0-2.5, 2.6-5.0, 5.1-7.5 and 7.6-10.0 </li>       \n",
    "    </ul>\n",
    "</ul>\n",
    "To practice, we will transform the categorical columns <i>State</i> using one-hot-encoding. <br>\n",
    "This can be done using the in-built function of <b>Pandas</b> called <code>get_dummies()</code>. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "func.iris = pd.get_dummies(func.iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing the <code>get_dummies()</code> function, we now transformed all categorical variables. <br>\n",
    "Now we will focus on the numerical variables, which can be transformed using normalization or scaling. <br>\n",
    "Scaling becomes especially important if you use distance dependend techniques, like for example clustering. <br>\n",
    "In these cases, without clustering, values that spread over a longer range can automatically be viewed as more important. <br>\n",
    "<br>\n",
    "\n",
    "For the Iris dataset the distribution of values over different parameters is quite similar. <br>\n",
    "For this reason it is not really necessary to apply scaling with regards to this dataset. <br>\n",
    "However we still apply it, as it will not decrease the quality of the data and this way you can learn to work with them as well. <br>\n",
    "<br>\n",
    "\n",
    "We will be using the Min-Max scaler, that transforms features by scaling them between the minimal and maximal value. <br>\n",
    "Despite the simplicity of the method, it is very often used in practice as it preserves the distribution while placing it on a set range. <br>\n",
    "The set range is between zero and one, which together with one-hot-encoding results in all values being in the range 0 to 1. <br>\n",
    "Min-Max scaling is applied by relating the value of x to the range between the minimal and maximal value found for that parameter. <br>\n",
    "This technique can be mathematically formulated as follows:<br>\n",
    "<br>\n",
    "$x_{new} = \\frac{x - x_{min}}{x_{max} - x_{min}} * (x_{max} - x_{min}) + x_{min}$\n",
    "\n",
    "<!-- Since we now don't have any categorical variables in the dataset anymore, the only transformation can be done on the numerical values. We could transform the data using the term of normalization or scaling. It is not strictly necessary to implement this in this dataset, but we do this to introduce you to all the techniques. Also some models need scaling because they are distances depend, meaning larger values are more dominant.\n",
    "<br>\n",
    "We'll be using min-max scaler that transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one. <br>\n",
    "<br>\n",
    "$X_{new} = \\frac{X - x_{min}}{x_{max} - x_{min}} * (max - min) + min$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Robust scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit to data, then scale it.\n",
    "func.iris[list(func.iris.columns)] = scaler.fit_transform(func.iris[list(func.iris.columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7> <b> -- B4. Data (Feature) Selection -- </b> </h7>\n",
    "\n",
    "As previously stated, the more parameters you have the complexer the Machine Learning (ML) has to be. <br>\n",
    "The increased complexity will thereafter result in more computing power needed to train and use the model. <br>\n",
    "Besides this, too much values can even worsen te performance of the trained model. <br>\n",
    "An example of this is with highly correlated values, as they are present multiple times, the model will unintendetly focus on this value more. <br>\n",
    "This will result in an output biased towards the correlated value, which is often further away from the actual truth.<br>\n",
    "<br>\n",
    "To prevent the downside of having too much features, Data (Feature) Selection is applied. <br>\n",
    "There are many approaches to do this, of which the following are often used: <br>\n",
    "<ul>\n",
    "    <li>Decide which features suit the task and algorithm based on Phase 1 and 2: Business and Data Understanding.</li>\n",
    "    <li>Apply <b><i>Ablation</i></b>, which implies iteratively trying different subsets of features and evaluate model performance.</li>\n",
    "    <li>Let the ML algorithm decide on the features, which can be done by looking at feature importances or through so-called <b>autoML</b>. </li>   \n",
    "</ul>\n",
    "Due to the simplicity of the task and dataset, approach 1 based on business and data understanding will be sufficient. <br>\n",
    "First, based on business understanding, we can conclude that the columns <i>state_wet</i> and <i>state_dry</i> will not have added value to the task. <br>\n",
    "Thereafter, looking at data understanding, we saw that the <i>petal length (cm)</i> and <i>petal width (cm)</i> were highly correlated. <br>\n",
    "<br>\n",
    "To prevent unnecessary model complexity, it is your task to:\n",
    "<ol>\n",
    "    <li> Remove <i>petal width (cm), state_wet, and state_dry</i> from the dataset. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns\n",
    "func.iris = ...\n",
    "\n",
    "func.execute_function(exercise=\"B4-1\", answer=func.iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7> <b> -- B5. Data Reduction -- </b> </h7>\n",
    "\n",
    "The final step is the transformation of data into a corrected, ordered, and simplified form. The purpose of data reduction can be two-fold:\n",
    "<ul>\n",
    "    <li> Reduce the number of data records by eliminating invalid data or produce summary data. </li>\n",
    "    <li> Produce aggregated statistics at different levels of detail for various applications.  </li>\n",
    "</ul>\n",
    "<br>\n",
    "As the used data is not that complex, we will leave this step as it is for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the execution of <b>data preprocessing</b>, the created Pandas DataFrame will be saved. <br>\n",
    "The backend functionalities enclosed within the <code>func</code> class contains a function to do this. <br>\n",
    "Simply run the code below and your data will be saved for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.save_preprocessed_dataframe(preprocessed_df=func.iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3> C. Modelling </h3>\n",
    "<p>\n",
    "After execution of all the preprocessing steps, the data is now ready to be used to create some Machine Learning (ML) models. <br>\n",
    "As shown in the Easy-LMS course, the field of Machine Learning can be subdivided into three categories: \n",
    "<ol>\n",
    "    <li><b>Unsupervised Learning</b> - Used to identify patterns in data sets containing data points that are neither classified nor labeled.</li>\n",
    "    <li><b>Supervised Learning</b> - Learn a function from labeled training data that maps an input to an output based on example input-output pairs. </li>\n",
    "    <li><b>Reinforcement Learning</b> - A learning method in which an agent is trained through interacting with an environment and receiving feedback. </li>\n",
    "</ol>\n",
    "Both Unsupervised as Supervised Learning are often used in practice, where Reinforcement Learning is not that integrated just yet. <br>\n",
    "For this reason, this notebook will primarily focus on these two types of modelling, starting with unsupervised learning.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7> <b> -- C1. Unsupervised Learning -- </b> </h7>\n",
    "\n",
    "The Iris dataset we have been working with so far does show the characteristics of different Iris flower species. <br>\n",
    "However they are not classified nor labelled, which leaves us in the unknown how many species there are and to what species specific records belong. <br>\n",
    "Solving this can be done using Unsupervised Learning, more specifically using a <b>distance-based clustering</b> algorithm. <br>\n",
    "<br>\n",
    "Unsupervised Learning algorithms, and thus clustering algorithms as well, are able to identify patterns in the available data. <br>\n",
    "Through this, we are thus able to group certain records together without a need for labaled data. <br>\n",
    "To let you experience the power of such techniques we will let you work with a method called <b>K-Means Clustering</b>. <br>\n",
    "K-Means Clustering is a classification algorithm that groups objects into <i>k</i> groups based on the relative distance between different datapoints.<br>\n",
    "For the creation of the K-Means algorithm we again return to the Machine Learning library Scikit-Learn, for which we refer to the [KMeans documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) for further reference. <br>\n",
    "<br>\n",
    "Initially we will set the number of clusters (<code>k</code>) to 2, feel free to change this value and see what happens in the plot visualisation. <br>\n",
    "You will notice that the clustering depends on the value set for the hyperparameter <code>k</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Number of clusters\n",
    "k = 2\n",
    "\n",
    "# K-Means clustering\n",
    "kmeans_model = KMeans(n_clusters=k, random_state=0)\n",
    "kmeans_model.fit(func.iris)\n",
    "\n",
    "# Add predicted values to a new column in the DataFrame\n",
    "func.iris['k_means'] = kmeans_model.predict(func.iris)\n",
    "\n",
    "# Plot the results in a 2D scatterplot\n",
    "scatter = func.iris.plot.scatter(x='sepal length (cm)', y='sepal width (cm)', c='k_means', colormap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7> <b> -- C2. Supervised Learning -- </b> </h7>\n",
    "\n",
    "In the case we have labelled data in our possession, supervised learning becomes possible. <br>\n",
    "Supervised learning can be used to predict either a continuous quantity (<b>regression</b>) or discrete class labels (<b>classification</b>). <br>\n",
    "A simple example for regression would be to predict the weather of tomorrow, where classification would predict if it will rain yes or no. <br>\n",
    "<br>\n",
    "The supervised learning task you will be working on is to classify to which species a given Iris flower belongs, which is a classification task. <br>\n",
    "To do this, we will extend the currently used data with a target column which describes the species of all records. <br>\n",
    "There exist a wide range of Supervised Learning algorithms, of which we will use the following:\n",
    "<ul>\n",
    "    <li> <b><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">K-nearest neighbours</a></b> - Classify a data point based on the class of its <code>k</code> closest neighbors, using majority voting. </li>\n",
    "    <li> <b><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\">Support Vector Machine</a></b> - Classify data points based on dividing the data space based on a maximized distance between certain vector points. </li>\n",
    "    <li> <b><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">RandomForestClassifier</a></b> - Classify data points through an ensemble of different decision tree classifiers, often using majority voting. </li>\n",
    "</ul>\n",
    "We selected these algorithms as they apply classification in significantly different ways. <br>\n",
    "A more in depth explanation of these methods can be found in the Scikit-learn documentation, which can be found following the hyperlinks behind their reference. <br>\n",
    "<br>\n",
    "Before being able to test these models we need to extend our current Iris dataset with the target labels and remove the clustered labels. <br>\n",
    "This will be done with a pre-written function, which is executed when running <code>func.prepare_supervised_learning()</code>. <br>\n",
    "The DataFrame you have been working on will be replaced with a new DataFrame, removing all preprocessing steps you have made. <br>\n",
    "Removal of these steps is mandatory, as during preprocessing we did not use a train-test split, resulting in data-leakage.\n",
    "<!-- We now start with the beginning DataFrame and therefore exclude all our preprocessing steps. In the original DataFrame we append the 'variety' column found in the <i>target.csv</i> to this copied DataFrame. Tip: look at this [module](https://pandas.pydata.org/docs/reference/api/pandas.concat.html#pandas.concat/) in Pandas. -->\n",
    "<!-- # Drop the 'k_means' result column\n",
    "fileName = \"iris_modified.csv\"\n",
    "iris_super = pd.read_csv(os.path.join(dataDir, fileName), delimiter=';')\n",
    "\n",
    "<!-- fileName = \"target.csv\"\n",
    "variety_column = pd.read_csv(os.path.join(dataDir, fileName), delimiter=';')\n",
    "iris_super = pd.concat([iris_super, variety_column], axis=1) --> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.prepare_supervised_learning()\n",
    "func.iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, a train-test split is needed to properly evaluate the performance of a Machine Learning (ML) model. <br>\n",
    "During <b>Unsupervised Learning</b> we did not make this split, for which reason we renounce the preprocessing steps we previously made. <br>\n",
    "The reason for this is data leakage, in which for example the missing values are imputed with the mean over all values. <br>\n",
    "This results in the model using data that it is not allowed to see, as visualized in the image below. <br>\n",
    "<br>\n",
    "![title](infoleak.png) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare our data, we would now have to split the data into two data sets and then repeat all preprocessing steps. <br>\n",
    "As this would be labour intensive, a smarter way to execute this is desired, called <b><i>Pipelines</i></b>. <br>\n",
    "Pipelines are created like a procedure, taking the data input through all the steps described upon creation of the pipeline. <br>\n",
    "<br>\n",
    "Again the library Scikit-learn helps us in creating these [pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), allowing to not only insert preprocessing steps but also actual models. <br>\n",
    "These pipelines have a fit, predict and score function integrated, which automatically ensure that data transformations are applied correctly. <br>\n",
    "Besides being a more professional and user-friendly way of programming, it also prevents any change of having data leakages in your process. <br>\n",
    "<br>\n",
    "Pipelines can combine multiple processing steps and a single estimator. <br>\n",
    "However, in some cases one pipeline is insufficient, for example when categorical data needs to be handled differently than numeric data. <br>\n",
    "If this is needed, separate pipelines can be made after which they are composed into a single pipeline, which we will show later on. <br>\n",
    "Despite their utility, pipelines are not able to replicate all preprocessing steps that we executed on the dataset (at least not without great effort). <br>\n",
    "The steps that require human intervention, like removing outliers, changing inconsistencies and removing certain columns are better to be done by hand. <br>\n",
    "<br>\n",
    "Please perform the following actions on the <code>func.iris</code> Dataframe:\n",
    "<ol>\n",
    "    <li> Change the metric system used by dividing the <i>sepal width (<b>mm</b>)</i> column by 10.</li> \n",
    "    <li> And change the column name to the correct reference <i>sepal width (<b>cm</b>)</i>.</li>\n",
    "    <li> Run <code>func.remove_0_in_outlier(outlier_value=actual_outlier)</code> to remove the outlier.</li> \n",
    "    <li> Remove <i>petal width (cm)</i> from the dataset.</li>\n",
    "    <li> View the first 5 rows of the Dataframe. </li>\n",
    "</ol>\n",
    "Be aware, in the following steps we <b>do not</b> remove the \"state\" column, which we previously did. <br>\n",
    "The reason for this is that we want to show you how to create such pipelines when you have both categorical and numerical variables. <br>\n",
    "We found that leaving in the \"state\" column will affect performance significantly, however we deemed this unimportant for the purpose of learning how to apply ML.\n",
    "\n",
    "<!-- We can create a test set and repeat all the previous data preprocessing steps. However, this is very labour intensive and ofcourse a smarter way of working is developed called <i>pipelines</i>. A pipeline is a combination of data transformation and learning algorithms. It has a fit, predict, and score method, just like any other learning algorithm and ensures that data transformations are applied correctly (no leakage). Such a pipeline combines multiple processing steps in a single estimator. However, it is order dependent since the last step should be the data transformation. In <b>B</b> we applied different operations to different columns. In pipelines we can also seperate this by creating different pipelines for each column. A pipeline doesn't include all the preprocessing steps; the outlier and Mm to Cm again needs to be done.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute preprocessing steps that require human intervention.\n",
    "func.iris = ...\n",
    "func.iris.head(5)\n",
    "\n",
    "func.execute_function(exercise=\"C1\", answer=func.iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.save_supervised_dataframe(supervised_df=func.iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have executed all manual transformations to the data, it is time to create the data pipeline. <br>\n",
    "As we did not remove the \"state\" column, we now need to create multiple pipelines. <br>\n",
    "First we will create a numerical and categorical pipeline, after which we combine both and append a classifier to it. <br>\n",
    "To do this we create a function named <code>create_pipeline()</code>, as this allows us to create these pipelines dynamically. <br>\n",
    "This will be useful for the way we intend to test the three classification models mentioned before in a looping fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def create_pipeline(imputer, scaler, encoder, clf) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Take the imputer, scaler, encoder and classifier and create and return a sklearn pipeline.\n",
    "\n",
    "    Args:\n",
    "        imputer (_type_): Imputer module, used to impute missing values in the data.\n",
    "        scaler (_type_): Scaling module, used to scale the data to a set range of values.\n",
    "        encoder (_type_): Encoding module, used to transform categorical values to a workable format.\n",
    "        clf (_type_): Classification model, which can be any model from the sklearn classification model catalog.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: Pipeline containing all preprocessing and classification models.\n",
    "    \"\"\"\n",
    "    # 2 sub-pipelines, one for numeric features, other for categorical ones\n",
    "    numeric_pipe = make_pipeline(imputer, scaler)\n",
    "    categorical_pipe = make_pipeline(encoder)\n",
    "\n",
    "    # Using categorical pipe for feature State, numeric pipe otherwise\n",
    "    preprocessor = make_column_transformer((categorical_pipe, [\"state\"]), \n",
    "                                            remainder=numeric_pipe)\n",
    "    \n",
    "    return Pipeline(steps=[('preprocess', preprocessor), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain things can be seen when looking at the created function above. <br>\n",
    "First, again we return to the Scikit-Learn package to help us create our Machine Learning [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). <br>\n",
    "Second, the pipeline consists of multiple subpipelines, namely a numeric and a categorical pipeline. <br>\n",
    "The numeric pipeline handles the numeric values and applies the [KNNImputer()](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) and [MinMaxScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html). <br>\n",
    "The categorical pipelines handles the categorical values, in this case the \"state\" column, and applies the Scikit-Learn in-built code for the [OneHotEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html). <br>\n",
    "In this case we use Scikit-Learn to apply One Hot Encoding, as the <b>Pandas</b> in-built function <code>get_dummies()</code> cannot be integrated into a Scikit-Learn pipeline. <br>\n",
    "Eventually both pipeliens are combined through the use of the Scikit-Learn in-built function [make_column_transformer()](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html). <br>\n",
    "As can be seen in the code and the documentation, you can combine multiple pipelines using this function by defining which columns flow through which pipelines. <br>\n",
    "Eventually we create our final pipeline, which defines the combined preprocessor pipeline and the classification model given to the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of the supervised learning method is in a big way dependent on de amount of data that is available. <br>\n",
    "As we have to split the data into a train and test set, the set used for training the model also decreases in size. <br>\n",
    "For this reason it is a must to put this data to good use to get a reliable evaluation of the performance of different models. <br>\n",
    "<br>\n",
    "Within the Iris dataset we have been using only 150 rows are present, which is not much when training Machine Learning (ML) models. <br>\n",
    "For this reason we will intelligently create train-test splits that will allow us to utilize the entire data for model evaluation. <br>\n",
    "The type of evaluation we will be using is called <b>K-fold Cross-Validation</b>, which is a technique to train and test models based on iteratively different subsets of the data. <br>\n",
    "The process of cross-validation using five iterations is shown in the following visualisation.<br>\n",
    "<br>\n",
    "![title](cross-val.png) <br>\n",
    "<br>\n",
    "Within this visualisation (<code>K</code> = 5), which implies that five iterations will be used to evaluate the models.<br>\n",
    "The score of the model is the mean score over all five iterations, which is returned as the final score. <br>\n",
    "Below we will use the Scikit-Learn in-built function for [K-fold Cross-Validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html). <br>\n",
    "This function will automatically execute the given number of fold (<code>cv</code>), which we set to 5, and return the attained performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create preprocessing steps into variables\n",
    "imputer = KNNImputer()\n",
    "scaler = MinMaxScaler()\n",
    "encoder = OneHotEncoder()\n",
    "classifiers = [\n",
    "     KNeighborsClassifier(), \n",
    "     SVC(random_state=0), \n",
    "     RandomForestClassifier(random_state=0)\n",
    "     ]\n",
    "\n",
    "# Split data into independent variables (X) and dependent variable (y)\n",
    "X = func.iris.copy().drop('target', axis=1)\n",
    "y = func.iris['target']\n",
    "\n",
    "# Test each different classifier\n",
    "for clf in classifiers:\n",
    "     # Combine with learning algorithm in another pipeline\n",
    "     pipe = create_pipeline(imputer, scaler, encoder, clf)\n",
    "\n",
    "     scores = cross_val_score(\n",
    "          estimator=pipe, \n",
    "          X=X, y=y,\n",
    "          cv=5)\n",
    "     print('{:>25} | Test Scores: {:>25} | Mean Test Score: {:.3f} | Standard deviation: {:.3f}'.format(pipe['clf'].__class__.__name__, str(['{:.3f}'.format(round(s, 3)) for s in scores]), round(scores.mean(), 3), round(scores.std(), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned values from <b>K-fold Cross Validation</b> need to be interpreted in several ways to get a complete view of the model performances. <br>\n",
    "First the individual test scores are shown, which show an overview how well every model performed on different iterations. <br>\n",
    "In itself this does not say much, but what needs to be looked at is the mean and standard deviation of these scores. <br>\n",
    "<br>\n",
    "A Machine Learning model has two distinct features which are important evaluation metrics, namely performance and robustness. <br>\n",
    "Performance can be compared using the mean test scores, for which a high score implies a good performing model. <br>\n",
    "The robustness however is in some cases as important as the performance, which is described in the standard deviation of all test scores. <br>\n",
    "If the standard deviation is high, the test scores for different iterations (folds) is significantly inconsistent. <br>\n",
    "This means that the performance of the model is dependent on which data you feed it, which is of course not desired as it shows to be unstable in practice. <br>\n",
    "<br>\n",
    "Please use the above scores to select the best performing model, which we will take through the final phase of the CRISP-DM model: <b>Evaluation</b>. <br>\n",
    "For the best performing model we create another dataframe called <code>df</code>. <br>\n",
    "Before going to the final phase, you need to:\n",
    "<ol>\n",
    "    <li> Remove the \"state\" column from the dataframe <code>df</code>. And specify the new X and y values. </li>\n",
    "    <li> Create a function called <code>best_model_pipeline</code> that returns a pipeline for the cleaned dataframe (with state removed) with the best performing model. </li>\n",
    "    <li> Run <code>func.save_pipeline_for_later_evaluation(df=df, pipeline=best_model_pipe)</code> and make sure your classifier has the argument <code>random_state=0</code>. </li>\n",
    "    <li> And finally run a K-fold Cross Validation with <code>K=4</code>. </li> \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = func.iris.copy(deep=True)\n",
    "\n",
    "df = ...\n",
    "\n",
    "X_new = ...\n",
    "y_new = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_pipeline(imputer, scaler, clf) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Take the imputer, scaler, and classifier and create and return a sklearn pipeline.\n",
    "\n",
    "    Args:\n",
    "        imputer (_type_): Imputer module, used to impute missing values in the data.\n",
    "        scaler (_type_): Scaling module, used to scale the data to a set range of values.\n",
    "        clf (_type_): Classification model, which can be any model from the sklearn classification model catalog.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: Pipeline containing all preprocessing and classification models.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the created pipeline function and run the save_pipeline_for_later_evaluation\n",
    "best_model_pipe = best_model_pipeline(KNNImputer(), MinMaxScaler(), RandomForestClassifier(random_state=0))\n",
    "\n",
    "func.save_pipeline_for_later_evaluation(df=df, pipeline=best_model_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the scores from the cross validation\n",
    "scores_best_model_pipe = ...\n",
    "print('{:>25} | Test Scores: {:>25} | Mean Test Score: {:.3f} | Standard deviation: {:.3f}'.format(best_model_pipe['clf'].__class__.__name__, str(['{:.3f}'.format(round(s, 3)) for s in scores_best_model_pipe]), round(scores_best_model_pipe.mean(), 3), round(scores_best_model_pipe.std(), 3)))\n",
    "\n",
    "func.execute_function(exercise=\"C2\", answer=round(scores_best_model_pipe.mean(), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3> D. Evaluation </h3>\n",
    "Normally when you work through the CRISP-DM model you work through it with only a single Machine Learning (ML) model. <br>\n",
    "However, in this module, we actually created both a Unsupervised Learning model (KMeans) and multiple Supervised Learning models. <br>\n",
    "On the backend we saved the data used for Unsupervised Learning, so we can re-use it here. <br>\n",
    "If you would have evaluated these models separately you would not face this issue, but to maintain the CRISP-DM workflow we perceived this to be the best way to show it to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7> <b> -- D1. Unsupervised Learning: KMeans Clustering -- </b> </h7>\n",
    "\n",
    "First we will focus on evaluating the performance of the Unsupervised Learning model we created earlier using [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) clustering. <br>\n",
    "The evaluation will imply defining the number of clusters that are present in the data, and fine-tuning this to optimize the output of the model. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve unsupervised dataset and remove k_means\n",
    "unsupervised_iris = func.get_unsupervised_dataset()\n",
    "unsupervised_iris.drop('k_means', axis=1, inplace=True)\n",
    "unsupervised_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the number of clusters used in K-Means clustering can be done using the <b>Elbow Criterion</b>. <br>\n",
    "The idea of the Elbow Criterion method is to choose a number of clusters (<code>k</code>) based on a visualisation of the Sum of Squared Errors (SSE). <br>\n",
    "The SSE is also called the Within Cluster Variation or the Inertia of the model, and it indicates the variation of the data points that belong to a single cluster. <br>\n",
    "The SSE is defined as the sum of the squared distance between each member of the cluster and its centroid. <br>\n",
    "We therefore normalized the data because it calculates the distance between the features of a variable and the centroid. <br>\n",
    "If we don't normalize the data when using unsupervised learning methods, there could be a distance bias. <br>\n",
    "<br>\n",
    "The Elbow plot, which is a visualisation of the SSE for different amounts of clusters, will always show a decreasing value as the amount increases. <br>\n",
    "However, at some point the decrease of the SSE seems to slow down, which might resemble an arm and an elbow. <br>\n",
    "The optimal number of clusters is at the point of the elbow in the plot, so where the speed of decline seems to slow down significantly. <br>\n",
    "As this is quite a subjective way to determine the amount of clusters, we would recommend that if you have doubt round up the amount of clusters. <br>\n",
    "<br>\n",
    "Below we will test different amounts of clusters, varying between 1 and 8. <br>\n",
    "We will use the <b>matplotlib</b> function <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\">plot()</a> to visualise the SSE over different amounts of clusters. <br>\n",
    "Therafter we will decide if the earlier chosen 2 clusters was optimal, or if another number of clusters would have been better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "WithinClusterVariation = []\n",
    "for k in range(1,8):\n",
    "    kmean_model = KMeans(n_clusters=k, random_state=0)\n",
    "    kmean_model.fit(unsupervised_iris)\n",
    "    WithinClusterVariation.append(kmean_model.inertia_)\n",
    "\n",
    "def plot_clusters(K, WCV):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(K, WCV, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal numbers of clusters')\n",
    "    plt.show()\n",
    "\n",
    "plot_clusters(K=range(1,8), WCV=WithinClusterVariation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot above, what do you think is the optimal number of clusters? <br>\n",
    "<ol>\n",
    "    <li> Please specify your best choice for the optimal number of cluster.\n",
    "</ol>\n",
    "As we said earlier, if you are unsure about two values, pick het highest value. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimal number of clusters found in the Elbow plot\n",
    "optimal_k_clusters = ...\n",
    "\n",
    "func.execute_function(exercise=\"D1\", answer=optimal_k_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7> <b> -- D2. Supervised Learning: Classification model -- </b> </h7>\n",
    "\n",
    "For supervised learning there are a lot of possibilities to evaluate the performance of the models you created. <br>\n",
    "With classification most often used way is to use a so-called <b>Confusion Matrix</b>, which shows the predicted and true labels. <br>\n",
    "Within a confusion matrix four classifications are possible, described below for a classification with two categories True and False. \n",
    "<ul>\n",
    "    <li><b>True Positive (TP)</b> - The value is <u>correctly</u> predicted to be True.</li>\n",
    "    <li><b>False Positive (FP)</b> - The value is <u>incorrectly</u> predicted to be True.</li>\n",
    "    <li><b>True Negative (TN)</b> - The value is <u>correctly</u> predicted to be False.</li>\n",
    "    <li><b>False Negative (FN)</b> - The value is <u>incorrectly</u> predicted to be False.</li>\n",
    "</ul>\n",
    "True and False in a classification task can be viewed broadly, for example in classifying if an image shows an apple. <br>\n",
    "If an apple is shown in the image and it is predicted this way it concerns a True Positive (TP). <br>\n",
    "If the prediction said classified it to be False, that no apple is in the image, than it would be a False Negative (FN) and so on. <br>\n",
    "<br>\n",
    "For multi-class classification, with more than two classes, the same mechanics apply. <br>\n",
    "If we have to classify if the image either shows an apple, pear or strawberry the following classifications are possible. <br>\n",
    "We view these classification from one class, take the apple for this example.\n",
    "<ul>\n",
    "    <li><b>True Positive (TP)</b> - It is classified to show an apple when an apple is shown in the image.</li>\n",
    "    <li><b>False Positive (FP)</b> - It is classified to show an apple when either a pear or strawberry is shown in the image.</li>\n",
    "    <li><b>True Negative (TN)</b> - It is classified to show either a pear or strawberry when indeed no applie is shown.</li>\n",
    "    <li><b>False Negative (FN)</b> - It is classified to show either a pear or stawberry when actually an apply is shown.</li>\n",
    "</ul>\n",
    "Below you will use the Scikit-Learn in-built function <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\">ConfusionMatrixDisplay</a>, which creates a Confusion Matrix based on the predicted and true labels. <br>\n",
    "Within the function the display labels are given, which are the actual names of the plant species. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_predictions = []\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "pipe = create_pipeline(imputer, scaler, encoder, clf)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true=y_test, \n",
    "        y_pred=y_pred, \n",
    "        display_labels=['Setosa', 'Versicolor', 'Virginica'], \n",
    "        xticks_rotation=\"vertical\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides a visual representation of the predictions, the Confusion Matrix can actually be used to deduce actual performance metrics. <br>\n",
    "These metrics will show a trade-off between all the categories (TP, FP, TN, FN) and return a single numerical value. <br>\n",
    "The reason different metrics are available is that with different tasks there is a different need for evaluation. <br>\n",
    "For example, in our previous fruit example it does not matter if you make either a FP or a FN, they both are equally bad. <br>\n",
    "But if you are for example predicting if a patient has cancer based on a given set of values, a FN can be way more damaging than a FP. <br>\n",
    "<br>\n",
    "The following metric can be deduced from a Confusion Matrix:\n",
    "<ul>\n",
    "    <li><b>Precision:</b> - The fraction of True predictions where actually True. </li>\n",
    "    <li><b>Recall:</b> - The fraction of actually True labels where predicted to be True. </li>\n",
    "    <li><b>F1-Score:</b> - The harmonic mean of precision and recall, comparing one class to all other classes. </li>\n",
    "</ul>\n",
    "To evaluate these metrics we will us the Scikit-Learn in-built function <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\">classification_report</a>. <br>\n",
    "Calling this function will automatically calculate all three mentioned metrics besides the support, which show the number of samples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=y_pred, \n",
    "    target_names=['Setosa', 'Versicolor', 'Virginica']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluding from the classification report it can be seen that all metrics show quite good performance. <br>\n",
    "It can be seen that for the species Virginica the performance is somewhat lower on all three metrics. <br>\n",
    "However, due to the sheer size of the dataset this might just be a single difficult example that decreases the model performance. <br>\n",
    "<br>\n",
    "To improve model performance we would need to increase the size of the dataset. <br>\n",
    "As can be seen in the confusion matrix, two predictions faulty predict either the Versicolor or the Virginica class. <br>\n",
    "More data might result in the model being able to have a clearer distinction between these species, resulting in increased performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1> Congratulations!! </h1>\n",
    "<p>\n",
    "You have succesfully created your first Machine Learning (ML) models and worked all the way through the CRISP-DM model. <br>\n",
    "A lot of code was already pre-written, as we perceived from our studies that this is the best way to learn this process. <br>\n",
    "However, the parts that you had to write will be checked using some pre-defined test again. <br>\n",
    "To kick off these tests, you only have to push this directory to the main Git branch. <br>\n",
    "This can be done using the following steps:\n",
    "<ol>\n",
    "    <li> Open the command prompt in VS Code by pressing <b>CTRL</b> + <b>`</b>. </li>\n",
    "    <li> Write the command <code>git status</code> to check which differences there are between your current and the master branch. </li>\n",
    "    <li> Write the command <code>git add .\\Modules\\M4_ML\\</code> to add the main notebook to the staging lane. </li>\n",
    "    <li> Write the command <code>git commit -m \"<i>[INSERT COMMIT MESSAGE]</i>\"</code> to commit the staged changes and adding a descriptive commit message. </li>\n",
    "    <li> Finally, write the command <code>git push</code> to push the staged changes to the master branch. </li>\n",
    "    <li> Upon pushing the changes, the PyTest modules will be run to check your answers, for which an overview is generated in your Github </li>\n",
    "    <li> \n",
    "        Upon pushing the changes, the PyTest modules will be run to check your answers, for which an overview is generated in your Github \n",
    "        <ol>\n",
    "            <li> View the results by heading to Github.com, open your forked repository and go to Actions. </li>\n",
    "            <li> Upon first opening the Actions page it can be that you have to enable it first, if so just press \"I understand my workflows, go ahead and enable them\" button. </li>\n",
    "            <li> Within the <b>\"All workflows\"</b> frame you will find a workflow run that has the same name as the commit message used to push your answers. </li>\n",
    "            <li> \n",
    "                In front of  the workflow there can be either one of three things:\n",
    "                <ul>\n",
    "                    <li> <b>A yellow circle</b> - Meaning that the tests are still running. </li>\n",
    "                    <li> <b>A green check mark</b> - Meaning that all tests were successful and your code is written perfectly! </li>\n",
    "                    <li> <b>A red cross</b> - Meaning that a mistake is found within your code. </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li> You can open the workflow run to view the details by clicking on the name of the workflow run (which is the commit message you wrote). </li>\n",
    "            <li> After opening the workflow run, you can go to the details by clicking the white box that contains the mark and the text <b>\"run\"</b>. </li>\n",
    "            <li> If a red cross is shown, meaning there is a mistake, the mistake can be found by navigating to (and opening) the <b>\"PyTest\"</b> section. </li>\n",
    "            <li> Below the <b>\"____test_results____\"</b> you can find the <b>\"AssertionError:\"</b> which shows you where in your code the (first) mistake can be found and what the mistake is. </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li> If a mistake was found, you head back to your code, fix the mistake using the hint shown behind the <b>AssertionError</b> and push your answers again. This will kickstart another test round. </li>\n",
    "</ol> \n",
    "If all tests are passed you are allowed to continue to our next module <b>Module 5: Application Programming Interface (API) - Basics</b>. <br>\n",
    "Feel free to return to this module when you need some example code to develop your ML models in the future, for example in the final assignment.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e01e7986e68b333fe809ea651447c5e875e888263704d380ff78c5e0510cf1f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
